#!/usr/bin/env python

import argparse
import pathlib
import tempfile
import os

import numpy as np
import onnx
import onnxruntime as ort
from onnx import TensorProto
from onnx import numpy_helper
from onnx import helper


# prase arguments
parser = argparse.ArgumentParser(description='Dump all the intermediate output to files')
parser.add_argument('model', type=pathlib.Path, help='ONNX model path')
parser.add_argument('inputs', nargs='*', type=argparse.FileType('rb'), help='input .pb files')
parser.add_argument('-o', nargs='?', default='connx.log', type=pathlib.Path, help='output directory to store intermediate outputs')
args = parser.parse_args()

# validate onnx model
model = onnx.load(args.model)
onnx.checker.check_model(model)

# prepare input tensors
inputs = {}
for input, file in zip(model.graph.input, args.inputs):
    proto = TensorProto()
    proto.ParseFromString(file.read())

    inputs[input.name] = numpy_helper.to_array(proto)

# get output names
outputs = []
files = []

for node in model.graph.node:
    for i, name in zip(range(64), node.output):
        outputs.append(name)
        files.append(f'{name}_{i}')

# modify onnx
while len(model.graph.output) > 0:
    model.graph.output.pop()

for output in outputs:
    proto = helper.ValueInfoProto()
    proto.name = output
    model.graph.output.extend([proto])

path = next(tempfile._get_candidate_names()) + '.onnx'
onnx.save(model, path)

# load model
session = ort.InferenceSession(path)

# remove temporary onnx
os.remove(path)

# run
outputs = session.run(outputs, inputs)

# output to directory
os.makedirs(args.o, exist_ok=True)
for file, output in zip(files, outputs):
    path = os.path.join(args.o, file.replace('/', '_'))
    np.save(path, output)
